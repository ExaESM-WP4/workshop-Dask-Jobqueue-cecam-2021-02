{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilience\n",
    "\n",
    "Dask is able to [cope with disappearing workers](http://distributed.dask.org/en/latest/resilience.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Dask batch job workers\n",
    "\n",
    "We define a Dask jobqueue cluster with Dask workers that each have 4 CPUs and 24 GB of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, dask.distributed\n",
    "import dask_jobqueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = dask_jobqueue.SLURMCluster(\n",
    "\n",
    "    # Dask worker size\n",
    "    cores=4, memory='24GB',\n",
    "    processes=1, # Dask workers per job\n",
    "    \n",
    "    # SLURM job script things\n",
    "    queue='cluster', walltime='00:15:00',\n",
    "    \n",
    "    # Dask worker network and temporary storage\n",
    "    interface='ib0', local_directory='$TMPDIR',\n",
    ")\n",
    "\n",
    "client = dask.distributed.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<distributed.deploy.adaptive.Adaptive at 0x14aef03d1e50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trick: Adaptive mode will recover from stopped workers\n",
    "cluster.adapt(minimum_jobs=8, maximum_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.18.4.100:41705</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.18.4.100:8787/status' target='_blank'>http://172.18.4.100:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>32</li>\n",
       "  <li><b>Memory: </b>192.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.18.4.100:41705' processes=8 threads=32, memory=192.00 GB>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From here everything is (almost) the same\n",
    "\n",
    "We'll return the Dask array for `pi` and handle computation more explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, dask.array\n",
    "\n",
    "def calculate_pi(size_in_bytes, number_of_chunks):\n",
    "    \n",
    "    \"\"\"Calculate pi using a Monte Carlo method.\"\"\"\n",
    "    \n",
    "    array_shape = (int(size_in_bytes / 8 / 2), 2)\n",
    "    chunk_size = (int(array_shape[0] / number_of_chunks), 2)\n",
    "    \n",
    "    # 2D random positions array using dask.array\n",
    "    xy = dask.array.random.uniform(\n",
    "        low=0.0, high=1.0, size=array_shape,\n",
    "        # specify chunk size, i.e. task number\n",
    "        chunks=chunk_size )\n",
    "  \n",
    "    xy_inside_circle = (xy ** 2).sum(axis=1) < 1 # boolean\n",
    "\n",
    "    pi = 4 * xy_inside_circle.sum() / xy_inside_circle.size\n",
    "        \n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's calculate again...\n",
    "\n",
    "Note the `.compute()`. For this demo, we'll need to handle construction of the Dask graph and compute more explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 390 ms, sys: 26.7 ms, total: 417 ms\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%time pi = calculate_pi(size_in_bytes=10_000_000_000, number_of_chunks=100).compute() # 10 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.74 s, sys: 143 ms, total: 3.88 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%time pi = calculate_pi(size_in_bytes=200_000_000_000, number_of_chunks=500).compute() # 100 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative way for handling computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<truediv, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>\n"
     ]
    }
   ],
   "source": [
    "pi = calculate_pi(size_in_bytes=200_000_000_000, number_of_chunks=500)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future: pending, key: finalize-ccb02a8b7207ffa277805fddd752597d>\n"
     ]
    }
   ],
   "source": [
    "pi = client.compute(\n",
    "    pi\n",
    ")\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14159670112\n"
     ]
    }
   ],
   "source": [
    "print(pi.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happens if a worker dies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll find out all \"our\" job ids, mark a few of them non-preemptible, filter for the preemptible jobs, and define a function to kill one randomly selected preemptible job.\n",
    "\n",
    "Note that here, we won't use any pre-emptibility that may be provided by the batch scheduler, but just pick jobids by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_jobs():\n",
    "    current_jobs = !squeue | grep R | grep $USER | grep dask | awk '{print $1}'\n",
    "    return current_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['55692', '55685', '55686', '55687']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_preemptible_jobs = get_current_jobs()[:4]\n",
    "non_preemptible_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preemptible_jobs():\n",
    "    return list(filter(lambda j: j not in non_preemptible_jobs, get_current_jobs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['55688', '55689', '55690', '55691']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_preemptible_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a way of randomly killing a preemptible job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def kill_random_preemptible_job():\n",
    "    preemptible_jobs = get_preemptible_jobs()\n",
    "    if preemptible_jobs:\n",
    "        worker_to_kill = random.choice(preemptible_jobs)\n",
    "        print(f\"will cancel job {worker_to_kill}\")\n",
    "        !scancel {worker_to_kill}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['55688', '55689', '55690', '55691']\n",
      "will cancel job 55689\n",
      "['55688', '55690', '55691']\n"
     ]
    }
   ],
   "source": [
    "print(get_preemptible_jobs())\n",
    "kill_random_preemptible_job()\n",
    "sleep(1)\n",
    "print(get_preemptible_jobs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n",
      "             55371   cluster jupyterl smomw122  R      46:36      1 neshcl100 \n",
      "             55692   cluster dask-wor smomw122  R       3:18      1 neshcl103 \n",
      "             55685   cluster dask-wor smomw122  R       3:21      1 neshcl230 \n",
      "             55686   cluster dask-wor smomw122  R       3:21      1 neshcl251 \n",
      "             55687   cluster dask-wor smomw122  R       3:21      1 neshcl251 \n",
      "             55688   cluster dask-wor smomw122  R       3:21      1 neshcl266 \n",
      "             55690   cluster dask-wor smomw122  R       3:21      1 neshcl266 \n",
      "             55691   cluster dask-wor smomw122  R       3:21      1 neshcl103 \n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start a computation with disappearing workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 8 B </td> <td> 8 B </td></tr>\n",
       "    <tr><th> Shape </th><td> () </td> <td> () </td></tr>\n",
       "    <tr><th> Count </th><td> 25336 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<truediv, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi = calculate_pi(\n",
    "    size_in_bytes=500_000_000_000, number_of_chunks=4_000\n",
    ")\n",
    "display(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future: pending, key: finalize-240b411f1edd54aa1372b9cb30622640>\n"
     ]
    }
   ],
   "source": [
    "pi = client.compute(pi)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will cancel job 55695\n",
      "will cancel job 55690\n",
      "will cancel job 55696\n",
      "will cancel job 55691\n",
      "will cancel job 55688\n",
      "will cancel job 55697\n"
     ]
    }
   ],
   "source": [
    "sleep(5)\n",
    "\n",
    "while not pi.done():\n",
    "    kill_random_preemptible_job()\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future: error, key: finalize-240b411f1edd54aa1372b9cb30622640>\n"
     ]
    }
   ],
   "source": [
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pi.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What happened?\n",
    "\n",
    "The Dask scheduler keeps a suspiciousness counter for each task it manages.  Whenever a worker dies, all tasks that belong to the worker at the time of its death will have their suspiciousness increased by one. In doing so, the scheduler has no way of telling which exact task was responsible for the death of the worker and just flag all of them as bad.\n",
    "\n",
    "All tasks with suspiciousness `>= 3` (default) are considered bad and won't be rescheduled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dask more resilient\n",
    "\n",
    "We can increase the number of allowed failures.  Let's practically disable the threshold and re-do the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scheduler.allowed_failures = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Note that the above is internal API that we need to use to increase the number of allowed failures for now.  With the current Dask.distributed release that we can't, however, use with Dask jobqueue yet, this can be changed by changing the Dask configuration at runtime.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start a computation with disappearing workers again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 8 B </td> <td> 8 B </td></tr>\n",
       "    <tr><th> Shape </th><td> () </td> <td> () </td></tr>\n",
       "    <tr><th> Count </th><td> 25336 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<truediv, shape=(), dtype=float64, chunksize=(), chunktype=numpy.ndarray>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pi = calculate_pi(\n",
    "    size_in_bytes=500_000_000_000, number_of_chunks=4_000\n",
    ")\n",
    "display(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future: pending, key: finalize-a1c802aa626c0ded9bf48d09e5884114>\n"
     ]
    }
   ],
   "source": [
    "pi = client.compute(pi)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will cancel job 55702\n",
      "will cancel job 55701\n",
      "will cancel job 55699\n",
      "will cancel job 55698\n"
     ]
    }
   ],
   "source": [
    "sleep(5)\n",
    "\n",
    "while not pi.done():\n",
    "    kill_random_preemptible_job()\n",
    "    sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Future: finished, type: numpy.float64, key: finalize-a1c802aa626c0ded9bf48d09e5884114>\n"
     ]
    }
   ],
   "source": [
    "print(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.141595972864\n"
     ]
    }
   ],
   "source": [
    "print(pi.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
